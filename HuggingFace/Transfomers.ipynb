{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/vocab.txt (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f234df83790>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 85354477-9d59-4843-8e8c-550619fda259)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/vocab.txt\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/added_tokens.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f234df83d60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 2eef5c8d-dda8-4948-a40a-ca7b2f5ed744)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/special_tokens_map.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f234df808e0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: e705c236-f4e4-4f3d-b103-fb0123dff4dd)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f234df81180>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 48f54f23-0ccc-42af-b1c3-66034c490542)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bert-base-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-cased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m----> 3\u001b[0m tokenizer\u001b[39m=\u001b[39mBertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1838\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1833\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1834\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1835\u001b[0m     )\n\u001b[1;32m   1837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1838\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1839\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1841\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1842\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1843\u001b[0m     )\n\u001b[1;32m   1845\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1846\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'bert-base-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-cased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaichuanTokenizer(name_or_path='', vocab_size=125696, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/volume/nlp')\n",
    "from Baichuan2_7B_base.tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "vocab_file='/volume/nlp/Baichuan2_7B_base/tokenizer.model'\n",
    "tokenizer=BaichuanTokenizer(vocab_file)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=tokenizer.get_vocab()\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 2, 4, 6, 0, 7, 6, 1],\n",
      "        [0, 1, 4, 2, 1, 9, 4, 6],\n",
      "        [2, 3, 9, 2, 3, 3, 8, 1],\n",
      "        [2, 4, 3, 6, 1, 0, 7, 8],\n",
      "        [5, 6, 5, 4, 6, 2, 2, 6]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 2, 4, 6, 0, 7, 6, 1],\n",
       "         [4, 2, 4, 6, 0, 7, 6, 1],\n",
       "         [4, 2, 4, 6, 0, 7, 6, 1]],\n",
       "\n",
       "        [[0, 1, 4, 2, 1, 9, 4, 6],\n",
       "         [0, 1, 4, 2, 1, 9, 4, 6],\n",
       "         [0, 1, 4, 2, 1, 9, 4, 6]],\n",
       "\n",
       "        [[2, 3, 9, 2, 3, 3, 8, 1],\n",
       "         [2, 3, 9, 2, 3, 3, 8, 1],\n",
       "         [2, 3, 9, 2, 3, 3, 8, 1]],\n",
       "\n",
       "        [[2, 4, 3, 6, 1, 0, 7, 8],\n",
       "         [2, 4, 3, 6, 1, 0, 7, 8],\n",
       "         [2, 4, 3, 6, 1, 0, 7, 8]],\n",
       "\n",
       "        [[5, 6, 5, 4, 6, 2, 2, 6],\n",
       "         [5, 6, 5, 4, 6, 2, 2, 6],\n",
       "         [5, 6, 5, 4, 6, 2, 2, 6]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.randint(0,10,(5,8))\n",
    "b=torch.randint(0,10,(3,8))\n",
    "print(a)\n",
    "a[:,None,:].expand(5,3,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 0, 1, 0, 7, 7, 2, 7],\n",
      "        [4, 5, 1, 1, 0, 5, 5, 9],\n",
      "        [6, 1, 4, 3, 3, 5, 4, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[6, 0, 1, 0, 7, 7, 2, 7],\n",
       "         [4, 5, 1, 1, 0, 5, 5, 9],\n",
       "         [6, 1, 4, 3, 3, 5, 4, 9]],\n",
       "\n",
       "        [[6, 0, 1, 0, 7, 7, 2, 7],\n",
       "         [4, 5, 1, 1, 0, 5, 5, 9],\n",
       "         [6, 1, 4, 3, 3, 5, 4, 9]],\n",
       "\n",
       "        [[6, 0, 1, 0, 7, 7, 2, 7],\n",
       "         [4, 5, 1, 1, 0, 5, 5, 9],\n",
       "         [6, 1, 4, 3, 3, 5, 4, 9]],\n",
       "\n",
       "        [[6, 0, 1, 0, 7, 7, 2, 7],\n",
       "         [4, 5, 1, 1, 0, 5, 5, 9],\n",
       "         [6, 1, 4, 3, 3, 5, 4, 9]],\n",
       "\n",
       "        [[6, 0, 1, 0, 7, 7, 2, 7],\n",
       "         [4, 5, 1, 1, 0, 5, 5, 9],\n",
       "         [6, 1, 4, 3, 3, 5, 4, 9]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b)\n",
    "b[None,:,:].expand(5,3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.ones(2,2)\n",
    "n=torch.tensor([2,1])\n",
    "a*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f233a853330> >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp_tokenizer=spm.SentencePieceProcessor(model_file=vocab_file)\n",
    "sp_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer.IsByte(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弘', '成', '教育', '成立于', '2', '0', '0', '8', '年']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs='弘成教育成立于2008年'\n",
    "tokens=sp_tokenizer.Encode(inputs,out_type=str)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弘', '成', '教育', '成立于', '2', '0', '0', '8', '年']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs='弘成教育成立于2008年'\n",
    "tokens=tokenizer.tokenize(inputs)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sp_model_kwargs': {},\n",
       " 'init_inputs': (),\n",
       " 'init_kwargs': {'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       "  'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       "  'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       "  'pad_token': None,\n",
       "  'add_bos_token': True,\n",
       "  'add_eos_token': False,\n",
       "  'sp_model_kwargs': {},\n",
       "  'clean_up_tokenization_spaces': False},\n",
       " 'name_or_path': '',\n",
       " '_processor_class': None,\n",
       " 'model_max_length': 1000000000000000019884624838656,\n",
       " 'padding_side': 'right',\n",
       " 'truncation_side': 'right',\n",
       " 'model_input_names': ['input_ids', 'attention_mask'],\n",
       " 'clean_up_tokenization_spaces': False,\n",
       " 'split_special_tokens': False,\n",
       " 'deprecation_warnings': {},\n",
       " '_in_target_context_manager': False,\n",
       " '_bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " '_eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " '_unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " '_sep_token': None,\n",
       " '_pad_token': None,\n",
       " '_cls_token': None,\n",
       " '_mask_token': None,\n",
       " '_pad_token_type_id': 0,\n",
       " '_additional_special_tokens': [],\n",
       " 'verbose': True,\n",
       " 'added_tokens_encoder': {},\n",
       " 'added_tokens_decoder': {},\n",
       " 'unique_no_split_tokens': [],\n",
       " 'tokens_trie': <transformers.tokenization_utils.Trie at 0x7f2342298190>,\n",
       " '_decode_use_source_tokenizer': False,\n",
       " 'vocab_file': '/volume/nlp/Baichuan2_7B_base/tokenizer.model',\n",
       " 'add_bos_token': True,\n",
       " 'add_eos_token': False,\n",
       " 'sp_model': <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f2342298300> >}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_create_trie',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_bos_token',\n",
       " 'add_eos_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'slow_tokenizer_class',\n",
       " 'sp_model',\n",
       " 'sp_model_kwargs',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [2, 3, 2, 3],\n",
       "        [4, 6, 4, 6],\n",
       "        [6, 9, 6, 9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.arange(0,4)\n",
    "b=torch.tensor([2,3])\n",
    "res=torch.outer(a,b)\n",
    "torch.cat((res,res),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "         [4, 4, 4, 5, 5, 5, 6, 6, 6]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#1*2*9\n",
    "a=torch.tensor([[[1,1,1,2,2,2,3,3,3],[4,4,4,5,5,5,6,6,6]]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1],\n",
       "          [4, 4, 4]]],\n",
       "\n",
       "\n",
       "        [[[2, 2, 2],\n",
       "          [5, 5, 5]]],\n",
       "\n",
       "\n",
       "        [[[3, 3, 3],\n",
       "          [6, 6, 6]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unflatten(-1,(3,3)).unsqueeze(0).transpose(0,-2).squeeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "arr=torch.arange(24).reshape(2,3,4)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 4,  5,  6,  7],\n",
       "         [16, 17, 18, 19]],\n",
       "\n",
       "        [[ 8,  9, 10, 11],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 12], [4, 16], [8, 20]],\n",
       " [[1, 13], [5, 17], [9, 21]],\n",
       " [[2, 14], [6, 18], [10, 22]],\n",
       " [[3, 15], [7, 19], [11, 23]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.transpose(0,2)\n",
    "#手算 4*3*2\n",
    "[[[0,12],\n",
    "  [4,16],\n",
    "  [8,20]],\n",
    " [[1,13],\n",
    "  [5,17],\n",
    "  [9,21]],\n",
    " [[2,14],\n",
    "  [6,18],\n",
    "  [10,22]],\n",
    " [[3,15],\n",
    "  [7,19],\n",
    "  [11,23]],\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 4, 8], [1, 5, 9], [2, 6, 10], [3, 7, 11]],\n",
       " [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.transpose(1,2)\n",
    "#手算 2*4*3\n",
    "[[[0,4,8],\n",
    "  [1,5,9],\n",
    "  [2,6,10],\n",
    "  [3,7,11]],\n",
    " [[12,16,20],\n",
    "  [13,17,21],\n",
    "  [14,18,22],\n",
    "  [15,19,23]],\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0,  1,  2,  3,  4],\n",
       "           [ 5,  6,  7,  8,  9],\n",
       "           [10, 11, 12, 13, 14]],\n",
       "\n",
       "          [[15, 16, 17, 18, 19],\n",
       "           [20, 21, 22, 23, 24],\n",
       "           [25, 26, 27, 28, 29]]],\n",
       "\n",
       "\n",
       "         [[[30, 31, 32, 33, 34],\n",
       "           [35, 36, 37, 38, 39],\n",
       "           [40, 41, 42, 43, 44]],\n",
       "\n",
       "          [[45, 46, 47, 48, 49],\n",
       "           [50, 51, 52, 53, 54],\n",
       "           [55, 56, 57, 58, 59]]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#五维度 1*2*4*3*5 1*bs*seq_len*3*hidden_size\n",
    "brr=torch.arange(60).reshape(1,2,2,3,5)\n",
    "brr\n",
    "#直观理解这个矩阵：每一个二维矩阵中表示输入序列的一个词的q,k,v向量，每两个矩阵组成一个bs中的序列\n",
    "#目的是将q，k，v单独抽取出来，即一个bs中原来是2*3*5\n",
    "# [\n",
    "# [[q11,q12,q13,q14,q15],[k11,k12,k13,k14,k15],[v11,v12,v13,v14,v15]],\n",
    "# [[q21,q22,q23,q24,q25],[k21,k22,k23,k24,k25],[v21,v22,v23,v24,v25]]\n",
    "# ]\n",
    "#要变成3*2*5\n",
    "# [\n",
    "# [[q11,q12,q13,q14,q15],[q21,q22,q23,q24,q25]],\n",
    "# [[k11,k12,k13,k14,k15],[k21,k22,k23,k24,k25]],\n",
    "# [[v11,v12,v13,v14,v15],[v21,v22,v23,v24,v25]]\n",
    "# ]\n",
    "\n",
    "#根据之前的理论，旋转时不变的轴，那么其轴上对应值得顺序就不会发生变化，我们\n",
    "#的目的是将q,k,v分开来，所以显然应该将-2轴旋转到最前面，但是又不能改变bs轴\n",
    "#上的顺序，所以需要添加一个0轴，然后将-2轴旋转到最前面，然后将新的-2轴去掉\n",
    "\n",
    "#试试手算转置下面的五维矩阵的0，-2轴\n",
    "#3*2*2*1*5\n",
    "#旧-2轴上的序列很好确定，但是旧0轴只有一个值\n",
    "#所以新的-2轴应该只有一个元素，新的0轴有3个元素\n",
    "#要想更加人性化的写出结果，就需要确定每个元素的位置，所以应当按照从内到外的顺序来写\n",
    "#注意：计算过程中为了利于观察，省略外层的一对括号，用间隔来代替\n",
    "#首先可以确定-1和0轴，-1轴的顺序不变，那么最里面的向量一定保持整体变动\n",
    "#其次是0轴，所以可以写出基本框架（后面的过程可以看做将0轴上每个元素补充完整）\n",
    "#[0,1,2,3,4]，\n",
    "\n",
    "#[5,6,7,8,9]，\n",
    "\n",
    "#[10,11,12,13,14]\n",
    "#这三组分别是0轴上的开头元素，但不完整\n",
    "#接着新的-2轴是原来的0轴，由于原来0轴只有一个元素\n",
    "#则新的-2轴就是每个加个括号，即\n",
    "#[[0,1,2,3,4]]，\n",
    "\n",
    "#[[5,6,7,8,9]]，\n",
    "\n",
    "#[[10,11,12,13,14]]\n",
    "#然后-3轴，不变，原来的顺序是0-15,5-20,10-25，所以可以写出\n",
    "# [[[0,1,2,3,4]]\n",
    "# [[15,16,17,18,19]]],\n",
    "\n",
    "#[[[5,6,7,8,9]],\n",
    "#[[20,21,22,23,24]]],\n",
    "\n",
    "#[[[10,11,12,13,14]],\n",
    "#[[25,26,27,28,29]]],\n",
    "#接下来是1轴，不变，原来的顺序是0-30,5-35,10-40;15-45,20-50,25-55,所以可以写出\n",
    "# [[[0,1,2,3,4]]\n",
    "# [[15,16,17,18,19]]],\n",
    "\n",
    "# [[[30,31,32,33,34]]\n",
    "# [[45,46,47,48,49]]],\n",
    "########这个算一块########\n",
    "#[[[5,6,7,8,9]],\n",
    "#[[20,21,22,23,24]]],\n",
    "\n",
    "# [[[35,36,37,38,39]]\n",
    "# [[50,51,52,53,54]]],\n",
    "########这个算一块########\n",
    "#[[[10,11,12,13,14]]\n",
    "# [[25,26,27,28,29]]],\n",
    "\n",
    "# [[[40,41,42,43,44]]\n",
    "# [[55,56,57,58,59]]]\n",
    "########这个算一块########\n",
    "#最后0轴，最开始确定的框架就是0轴，所以加一个最外层的括号和间隔代表的括号就行了\n",
    "#结果应该是(注意区分单行空格和双行空格)\n",
    "#[\n",
    "# [\n",
    "# [[[0,1,2,3,4]]\n",
    "# [[15,16,17,18,19]]],\n",
    "\n",
    "# [[[30,31,32,33,34]]\n",
    "# [[45,46,47,48,49]]]\n",
    "# ],\n",
    "\n",
    "\n",
    "# [\n",
    "#[[[5,6,7,8,9]],\n",
    "#[[20,21,22,23,24]]],\n",
    "\n",
    "# [[[35,36,37,38,39]]\n",
    "# [[50,51,52,53,54]]],\n",
    "# ],\n",
    "\n",
    "\n",
    "# [\n",
    "#[[[10,11,12,13,14]]\n",
    "# [[25,26,27,28,29]]],\n",
    "\n",
    "# [[[40,41,42,43,44]]\n",
    "# [[55,56,57,58,59]]]\n",
    "# ],\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0,  1,  2,  3,  4]],\n",
       "\n",
       "          [[15, 16, 17, 18, 19]]],\n",
       "\n",
       "\n",
       "         [[[30, 31, 32, 33, 34]],\n",
       "\n",
       "          [[45, 46, 47, 48, 49]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 5,  6,  7,  8,  9]],\n",
       "\n",
       "          [[20, 21, 22, 23, 24]]],\n",
       "\n",
       "\n",
       "         [[[35, 36, 37, 38, 39]],\n",
       "\n",
       "          [[50, 51, 52, 53, 54]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[10, 11, 12, 13, 14]],\n",
       "\n",
       "          [[25, 26, 27, 28, 29]]],\n",
       "\n",
       "\n",
       "         [[[40, 41, 42, 43, 44]],\n",
       "\n",
       "          [[55, 56, 57, 58, 59]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#验证\n",
    "brr.transpose(0,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#总结：这是一种从介于几何和代数之间的一种理解方式，如果只用几何，在处理高维度的时候几乎无法使用\n",
    "#如果只用代数，适合编写代码，但是理解起来还是比较抽象（https://blog.csdn.net/qq_38290604/article/details/105390250）\n",
    "#本文从交换轴的角度来理解，交换轴的时候，不变的轴上的顺序是不变的，变动的轴上的顺序是变动的，可以较好的理解。\n",
    "#这种方式能够为了实现我们想要达到的目的，可以推算出应该如何交换轴，而不是从结果来理解为什么这样交换。\n",
    "#比如这里想要将q，k，v分开，为什么要先添加一个0轴，再交换-2轴和0轴(分开qkv，所以要将-2轴拿到最前，但是又不能破坏bs上的顺序，则加一个0维）。\n",
    "#另外，理解上述过程之后从实际意义上来理解结果，比如一开始是1*bs*seq_len*3*hidden_size\n",
    "#得到的结果是3*bs*seq_len*1*hidden_size\n",
    "#即我们有q,k,v三个4维张量,每个张量的第一维是bs，第二维是seq_len，第三维和第四维一起是[hidden_size]，\n",
    "#所以可以得到q就是(下标三个值分别表示bs,seq_len,hidden_size)\n",
    "#[[[q111,q112,q113,q114,q115],[q121,q122,q123,q124,q125]]],\n",
    "#[[[q211,q212,q213,q214,q215],[q221,q222,q223,q224,q225]]],\n",
    "#对应上面的例子就是\n",
    "#[[[[ 0,  1,  2,  3,  4]],[[15, 16, 17, 18, 19]]],\n",
    "# [[[30, 31, 32, 33, 34]],[[45, 46, 47, 48, 49]]]]\n",
    "#k,v同理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
